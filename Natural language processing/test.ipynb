{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHP3Bdf9FLwFPrTctcnDx0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nay-uku/studia/blob/main/Natural%20language%20processing/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWTDolRFRoYu",
        "outputId": "50077e6b-d78f-4b1d-9e1f-e648205619fb"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Pozycje ksiazkowe do analizy \n",
        "books=['carroll-alice.txt','shakespeare-hamlet.txt','austen-persuasion.txt', 'austen-sense.txt','melville-moby_dick.txt']\n",
        "\n",
        "#################################\n",
        "## \n",
        "## Preprocessing  \n",
        "## \n",
        "#################################\n",
        "\n",
        "#Korpus dokumentów: 5 -elementowa lista \n",
        "corpus = []\n",
        "\n",
        "for fileid in books:\n",
        "  #surowe dane tekstowe \n",
        "  txt = gutenberg.raw(fileids=fileid)\n",
        "  # Przekształć dane tekstowe do ciągu tokenów pisanych z małymi literami,\n",
        "  # usuwając wszystkie ciągi które nie mają charakteru alfanumerycznego: możesz użyć funkcji isalpha() , nltk.word_tokenize\n",
        "  data = [w for w in nltk.word_tokenize(txt) if w.isalpha()] #\n",
        " # print(data) \n",
        "  #Dodaj pozycję korpusu \n",
        "  corpus.append(' '.join(data)) #\n",
        "#print(corpus)\n",
        "\n",
        "#################################\n",
        "## \n",
        "## TFIDF  \n",
        "## \n",
        "#################################\n",
        "\n",
        "\n",
        "#Wykorzystaj TfidfVectorizer do zbudowania macierzy tfidf \n",
        "tfidf = TfidfVectorizer(smooth_idf=True ,stop_words='english')\n",
        "\n",
        "#macierz tf-idf \n",
        "tfs = tfidf.fit_transform(corpus) #\n",
        "#print(tfs)\n",
        "#lista słów odpowiadająca indeksom w macierzy tfs\n",
        "tokens = tfidf.get_feature_names() #\n",
        "#print(tokens)\n",
        "\n",
        "#podobienstwo dokumentow do siebie \n",
        "similarity_matrix = cosine_similarity(tfs, tfs)\n",
        "\n",
        "\n",
        "#################################\n",
        "## \n",
        "## LSI  \n",
        "## \n",
        "#################################\n",
        "\n",
        "# dokonaj dekompozycji zbudowanej macierzy wyodrębniając 3 tematy \n",
        "svd_model = TruncatedSVD(n_components = 3) \n",
        "\n",
        "u_matrix = svd_model.fit_transform(tfs)  #\n",
        "v_matrix = svd_model.components_ #\n",
        "\n",
        "# załóżmy, że interesuje nas temat  1   . Lista topic_tokens_weights powinna mieć długość odpowiadającą liczbie słów słowniku \n",
        "topic_index  = 1\n",
        "topic_tokens_weights = v_matrix[topic_index] #   \n",
        "#print(topic_tokens_weights)\n",
        "#sprawdzenie poprawnosci\n",
        "assert(len(tokens)==len(topic_tokens_weights))\n",
        "\n",
        "# wybierz indeksy odpowiadające dwóm tokenom o największej wadze w liście topic_tokens_weights \n",
        "ranked = np.argsort(topic_tokens_weights)\n",
        "largest_indices = ranked[:2] #\n",
        "\n",
        "top_tokens = [tokens[i] for i in largest_indices]\n",
        "print(top_tokens, largest_indices, topic_tokens_weights[largest_indices[0]], topic_tokens_weights[largest_indices[1]])\n",
        "#print(np.sort(topic_tokens_weights))\n",
        "##################################################\n",
        "##################################################\n",
        "##################################################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['elinor', 'alice'] [6087  495] -0.15596428899156062 -0.15420060482457223\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}