{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JhtaWefEx98",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "65893c73-63a1-4bdc-fa26-0dff8e88ed94"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# załadowanie środowiska\n",
        "env = gym.make(\"FrozenLake-v0\", is_slippery=False).env\n",
        "# reset środowiska do losowego stanu\n",
        "env.reset()\n",
        "\n",
        "# tworzenie QTable\n",
        "action_size = env.action_space.n # ilość akcji (kolumny)\n",
        "state_size = env.observation_space.n # ilość stanów (wiersze)\n",
        "qtable = np.zeros((state_size, action_size)) # inicjalizacja qtable zerami\n",
        "print(qtable)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZISbnUIo0fk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parametry\n",
        "episodes = 10000\n",
        "lr = 0.8\n",
        "steps = 99 # maksymalna liczba kroków na epokę\n",
        "gamma = 0.95 # współczynnik obniżenia\n",
        "\n",
        "# Parametry eksploracji \n",
        "eps = 1.0 # współczynnik eksploracji\n",
        "max_eps_prob = 1.0 # prawdopodobieństwo eksploracji na starcie\n",
        "min_eps_prob = 0.01 # najmniejsze prawdopodobieństwo eksploracji\n",
        "decay_rate = 0.005 # szybkość zaniku wykładniczego"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqyMtBXeuxHJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "6537c9ed-bf77-444a-a9ff-6972418442a0"
      },
      "source": [
        "# Lista nagród\n",
        "rewards = []\n",
        "# Pętla ucząca\n",
        "for episode in range(episodes):\n",
        "  state = env.reset() # reset gry - początkowe ustawienie\n",
        "  step = 0\n",
        "  done = False\n",
        "  total_rewards = 0\n",
        "  for step in range(steps):\n",
        "    # Wybranie akcji w obecnym stanie świata (s)\n",
        "    exp_exp_tradeoff = random.uniform(0,1) # Losowa wartość\n",
        "    # Jeżeli wylosowana wartość > eps, to wykonuj eksploitację \n",
        "    # (wzięcie największej wartości Q dla danego stanu)\n",
        "    if exp_exp_tradeoff > eps:\n",
        "      action = np.argmax(qtable[state,:]) # wybranie najwyżej wartości z wiersza\n",
        "    else:\n",
        "      action = env.action_space.sample() # eksploracja - wybierz losową akcję\n",
        "    # Dla wybranej akcji policz stan i nagrodę\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "    # Równanie Bellmana - aktualizacja wartości Q w qtable:\n",
        "    qtable[state, action] = qtable[state, action] \\\n",
        "     + lr * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "    # licznik nagród\n",
        "    total_rewards += reward\n",
        "    # aktualizacja stanu\n",
        "    state = new_state\n",
        "    # Przedwczesny koniec, porażka w grze:\n",
        "    if done == True:\n",
        "      break\n",
        "  # Redukcja epsilonu, aby z każdą sukcesywną epoką mieć mniej eksploracji,\n",
        "  # a więcej eksploitacji\n",
        "  eps = min_eps_prob + (max_eps_prob - min_eps_prob) * np.exp(-decay_rate\n",
        "                                                              * episode)\n",
        "  rewards.append(total_rewards)\n",
        "print(\"Training finished.\\n Average rewards per episode: \" + str(sum(rewards)/episodes)) \n",
        "print(qtable)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            " Average rewards per episode: 0.9661\n",
            "[[0.73509189 0.77378094 0.6983373  0.73509189]\n",
            " [0.73509189 0.         0.66183271 0.69752098]\n",
            " [0.69833177 0.49165312 0.40141551 0.45412746]\n",
            " [0.63584666 0.         0.         0.        ]\n",
            " [0.77378094 0.81450625 0.         0.73509189]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.9025     0.         0.63667347]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.81450625 0.         0.857375   0.77378094]\n",
            " [0.81450625 0.9025     0.9025     0.        ]\n",
            " [0.857375   0.95       0.         0.857375  ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.9025     0.95       0.857375  ]\n",
            " [0.9025     0.95       1.         0.9025    ]\n",
            " [0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piyCmEfP2IUR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "a584387b-e652-4338-db65-ebe298c5719f"
      },
      "source": [
        "# Agent grający w Frozen Lake\n",
        "env.reset()\n",
        "episodes, total_epochs = 100, 0\n",
        "rewards = []\n",
        "for episode in range(episodes):\n",
        "  state = env.reset()\n",
        "  step, total_rewards = 0, 0\n",
        "  done = False\n",
        "\n",
        "  for step in range(steps):\n",
        "    # Wybierz akcję (indeks qtable) z najwyższą wartość nagrody dla tego stanu\n",
        "    action = np.argmax(qtable[state,:])\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "    total_rewards += reward\n",
        "    if done:\n",
        "      # ostatni stan - agent dotarł do celu lub wpadł do dziury\n",
        "      total_epochs += step\n",
        "      break\n",
        "    state = new_state\n",
        "  rewards.append(total_rewards)\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs/episodes}\")\n",
        "print(f\"Average rewards per episode: {sum(rewards)/episodes}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 5.0\n",
            "Average rewards per episode: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq15A_zPTYQ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "3beff888-b636-4807-8cca-6e3a4a1f6297"
      },
      "source": [
        "'''\n",
        "  Wizualizacja\n",
        "'''\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "  env.render()\n",
        "  action = np.argmax(qtable[state])\n",
        "  state, reward, done, info = env.step(action)\n",
        "env.render()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB6ooVlrTbSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}